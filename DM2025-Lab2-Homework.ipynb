{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name: CHEN YU-WEN 陳伃汶\n",
    "\n",
    "Student ID: 61347093S\n",
    "\n",
    "GitHub ID: chenee0713\n",
    "\n",
    "Kaggle name: chen\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/kaggle_privatescore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project Report**\n",
    "\n",
    "-----\n",
    "\n",
    "## 1\\. Model Development (10 pts Required)\n",
    "\n",
    "In this competition, I developed a robust **Ensemble Learning Framework** combining three state-of-the-art Transformer models to classify Twitter emotions. The approach focuses on leveraging domain-specific pre-training and architectural diversity to maximize the Macro F1 Score.\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "Before implementing the model, I conducted a comprehensive **Exploratory Data Analysis (EDA)** to understand the dataset characteristics and inform my preprocessing strategy.\n",
    "\n",
    "#### **A. Dataset Analysis & Insights**\n",
    "\n",
    "**1. Label Distribution (Class Imbalance)**\n",
    "\n",
    "  * **Observation:** The dataset is highly imbalanced. *Joy* (49.7%) dominates the dataset, while *Disgust* (2.5%) and *Fear* (4.2%) are significantly underrepresented. The max imbalance ratio is **20.12:1**.\n",
    "\n",
    "   ![The 3.7:1 imbalance ratio (shown in right panel) necessitated balanced class weights to prevent model bias toward majority classes (joy, sadness).](./pics/label_distribution_enhanced.png)\n",
    "  \n",
    "  * **Decision:** This severe imbalance necessitates the use of **Computed Class Weights** in the loss function to prevent the model from ignoring minority classes.\n",
    "\n",
    "**2. Text Length Analysis**\n",
    "\n",
    "  * **Observation:** The average tweet length is **75.2 characters** (approx. 14 words). The 99th percentile falls at 173 characters.\n",
    "\n",
    "   ![The cumulative distribution (bottom right) shows MAX_LENGTH=128 tokens (≈512 chars) covers 99% of tweets, minimizing truncation while maintaining efficiency. Statistics table confirms median of 138 characters aligns with Twitter's legacy limit.](./pics/text_length_analysis_enhanced.png)  \n",
    "\n",
    "  * **Decision:** I selected a **MAX\\_LENGTH of 128 tokens**. As shown in the cumulative distribution, this length comfortably covers \\>99% of all samples without truncation, optimizing GPU memory usage while preserving information.\n",
    "\n",
    "**3. Content Analysis (Word Frequency & Hashtags)**\n",
    "\n",
    "  * **Hashtags:** 85% of tweets contain hashtags, they carry strong emotional signals (e.g., `#happy`, `#sad`, `#fear`).\n",
    "\n",
    "   ![85% of tweets contain hashtags (left panel), with emotion-specific patterns: joy/surprise use more (avg 2.5), anger/disgust fewer (avg 2.0). Top hashtags (#love, #happy, #sad) align with emotions, justifying their inclusion in model input.](./pics/hashtag_analysis_enhanced.png)  \n",
    "\n",
    "\n",
    "  * **Word Frequency:** After removing stopwords, the most frequent words (e.g., \"name\", \"like\", \"love\") appear across multiple emotions, but distinct patterns emerge in the tail.\n",
    "\n",
    "   ![Top words after stopword removal (e.g., \"love\", \"happy\", \"hate\", \"sad\") confirm our preprocessing preserves discriminative emotional vocabulary while removing noise.](./pics/word_frequency_analysis.png)  \n",
    "\n",
    "  * **Decision:** I decided to **preserve hashtags** and append them to the text context to capture these explicit emotional labels.\n",
    "\n",
    "**4. Word Clouds by Emotion**\n",
    "\n",
    "  * **Observation:** Visualizing the most common words for each emotion reveals distinct vocabularies. For instance, *Joy* is characterized by \"love\", \"thank\", and \"good\", while *Anger* is dominated by strong negative words like \"hate\" and \"stupid\".\n",
    "\n",
    "![Word clouds reveal strong label-content alignment: joy dominated by \"love\", \"happy\"; anger by \"hate\", \"people\"; sadness by \"sad\", \"miss\". This validates dataset quality and suggests high model discriminability.](./pics/wordclouds_by_emotion.png)  \n",
    "\n",
    "\n",
    "\n",
    "#### **B. Text Cleaning Strategy**\n",
    "\n",
    "Based on the analysis above, I implemented a **Model-Specific Cleaning Strategy** to preserve semantic information while reducing noise.\n",
    "\n",
    "1.  **Twitter-RoBERTa Preprocessing:**\n",
    "      * Replaced `@username` with the special token `@user`.\n",
    "      * Replaced URLs with `http`.\n",
    "      * *Reason:* This matches the pre-training objective of the Twitter-RoBERTa model, ensuring the tokenizer aligns with the input distribution.\n",
    "2.  **DeBERTa / XLM-RoBERTa Preprocessing:**\n",
    "      * Applied basic whitespace normalization (`r'\\s+'`) to remove extra spaces.\n",
    "      * Kept original text mostly intact to leverage their robust language understanding capabilities.\n",
    "3.  **Universal Rules:**\n",
    "      * **Preserved Emojis & Punctuation:** Symbols like `!` or `?` are critical intensity indicators in sentiment analysis.\n",
    "      * **Hashtag Integration:** Combined hashtags with the main text: `text + \" \" + \"#hashtag\"`.\n",
    "\n",
    "#### **C. Data Split**\n",
    "\n",
    "  * **Training set:** 15,919 tweets (70%)\n",
    "  * **Test set:** 6,823 tweets (30%)\n",
    "  * Used **Stratified K-Fold (N=5)** to ensure the distribution of minority classes (like *Disgust*) remains consistent across all training folds and prevents overfitting to specific data subsets.\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "Instead of manual feature extraction, I utilized the powerful embedding capabilities of Transformers, enhanced by the following strategies:\n",
    "\n",
    "  * **Tokenization:** Used model-specific AutoTokenizers with a `max_length` of **128 tokens** (based on the 99th percentile length analysis).\n",
    "  * **Class Balancing (Crucial Step):**\n",
    "      * Computed balanced class weights using `sklearn.utils.class_weight`.\n",
    "      * **Weight Injection:** Applied weights directly in the loss function: `CrossEntropyLoss(weight=class_weights)`.\n",
    "      * *Strategy Shift:* I initially experimented with clipping weights to [0.5, 3.0], but empirical results showed that allowing full, unclipped weights provided better recall for the rarest class (*Disgust*).\n",
    "  * **Ensemble Features:**\n",
    "      * Combined predictions from 3 diverse model architectures (Soft Voting).\n",
    "      * Averaged probability distributions from 5 folds per model (Total 15 inference passes per sample) to smooth out variance.\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "My final submission is a **Weighted Soft Voting Ensemble** of three distinct Transformer architectures. This \"Team of Experts\" approach leverages the unique strengths of each model:\n",
    "\n",
    "**1. Model A: `cardiffnlp/twitter-roberta-base-emotion` (The Domain Expert)**\n",
    "\n",
    "  * **Why:** Pre-trained specifically on \\~58M tweets and fine-tuned on emotion datasets. It understands internet slang, emojis, and Twitter syntax better than any generic model.\n",
    "  * **Configuration:** 125M parameters, specialized tokenizer.\n",
    "\n",
    "**2. Model B: `microsoft/deberta-v3-base` (The Architecture Expert)**\n",
    "\n",
    "  * **Why:** Uses \"disentangled attention\" and an enhanced mask decoder. It excels at understanding complex syntactic structures and negations (e.g., \"not happy\") which RoBERTa might miss.\n",
    "  * **Configuration:** 183M parameters.\n",
    "\n",
    "**3. Model C: `xlm-roberta-base` (The Generalist)**\n",
    "\n",
    "  * **Why:** A multilingual model trained on 100 languages. Added to increase diversity in the ensemble variance, helping to smooth out predictions and improve generalization on unseen test data.\n",
    "  * **Configuration:** 279M parameters.\n",
    "\n",
    "**Training Configuration:**\n",
    "\n",
    "  * **Optimizer:** AdamW with weight decay 0.01.\n",
    "  * **Learning Rate:** `2e-5` with a linear warmup (10%).\n",
    "  * **Batch Size:** 8 (constrained by GPU memory, but effective for generalization).\n",
    "  * **Epochs:** 6 with Early Stopping (`patience=3`).\n",
    "  * **Mixed Precision (FP16):** Enabled for training efficiency.\n",
    "\n",
    "**Ensemble Strategy:**\n",
    "The final prediction is derived from the weighted average of the softmax probabilities from all 3 models:\n",
    "$$P_{final} = 0.33 \\times P_{RoBERTa} + 0.33 \\times P_{DeBERTa} + 0.34 \\times P_{XLM}$$\n",
    "\n",
    "**Results:**\n",
    "\n",
    "  * Twitter-RoBERTa OOF F1: **0.6039**\n",
    "  * DeBERTa OOF F1: **0.5410**\n",
    "  * XLM-RoBERTa OOF F1: **0.5284**\n",
    "  * **3-Model Ensemble F1: 0.6929 (Kaggle Private Leaderboard)**\n",
    "\n",
    "-----\n",
    "\n",
    "## 2\\. Bonus Section (5 pts Optional)\n",
    "\n",
    "### 2.1 Mention Different Things I Tried\n",
    "\n",
    "During the development process, I conducted systematic ablation studies. Many experiments did *not* make it into the final model because they degraded performance. This trial-and-error process was vital:\n",
    "\n",
    "1.  **Sentiment vs. Emotion Models:**\n",
    "      * *Experiment:* Initially used `twitter-roberta-base-sentiment-latest`.\n",
    "      * *Result:* F1 Score stuck at \\~0.68.\n",
    "      * *Insight:* Sentiment models (Positive/Neutral/Negative) lack the granularity to distinguish between *Anger* and *Disgust*. Switching to the `base-emotion` checkpoint yielded an immediate boost.\n",
    "2.  **Data Augmentation (Back-Translation & Random Deletion):**\n",
    "      * *Experiment:* Used `nlpaug` to generate synthetic data for minority classes.\n",
    "      * *Result:* Performance dropped.\n",
    "      * *Insight:* Augmentation introduced semantic noise. In this specific task, \"clean\" real data was more valuable than \"noisy\" synthetic data. Transformer models are sensitive to the grammatical disruptions caused by random deletion.\n",
    "3.  **Weight Clipping:**\n",
    "      * *Experiment:* Clipped class weights to [0.5, 3.0] to prevent instability.\n",
    "      * *Result:* F1 score for *Disgust* dropped significantly.\n",
    "      * *Insight:* The imbalance is so severe (20:1) that minority classes actually needed the full, unclipped penalty weight (approx. 6.0+) to be learned effectively.\n",
    "4.  **Hyperparameter Tuning:**\n",
    "      * *Experiment:* Tested learning rates [1e-5, 2e-5, 3e-5].\n",
    "      * *Result:* `2e-5` proved optimal. `3e-5` caused unstable convergence in DeBERTa.\n",
    "\n",
    "### 2.2 Mention Insights I Gained\n",
    "\n",
    "  * **Domain-Specificity Wins:** The `twitter-roberta` model consistently outperformed larger generic models (like standard BERT) because its vocabulary matches the data distribution (tweets) perfectly. The specific preprocessing (keeping `@user` tokens) improved performance by \\~1%.\n",
    "  * **Macro F1 Sensitivity:** I learned that in Macro F1 optimization, **saving the minority class is the priority**. It is better to sacrifice 1% accuracy on the majority class (*Joy*) to gain 10% accuracy on the minority class (*Disgust*), as this trade-off significantly boosts the overall metric.\n",
    "  * **The Power of Ensembling:** While individual models scored around 0.68\\~0.69, the ensemble consistently pushed the score near 0.70. The heterogeneity of the models (Twitter-specific + General Transformer + Multilingual) allowed them to correct each other's blind spots.\n",
    "  * **Cross-Validation Reliability:** My local 5-fold CV OOF scores matched the Kaggle Private LB scores within a margin of $\\pm 0.01$, confirming that the Stratified K-Fold strategy effectively prevented overfitting and provided a trustworthy estimate of model performance.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Syntax:** `##` creates a secondary heading (H2).\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "**Syntax:** `###` creates a tertiary heading (H3).\n",
    "\n",
    "[Content for Preprocessing]\n",
    "\n",
    "**Example Syntax for Content:**\n",
    "*   **Bold text:** `**text**`\n",
    "*   *Italic text*: `*text*`\n",
    "*   Bullet point list:\n",
    "    * Item 1\n",
    "    * Item 2\n",
    "\n",
    "Markdown Syntax to Add Image: `![Description of the Image](./your_local_folder/name_of_the_image.png)`\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/example_md_img.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "[Content for Feature Engineering]\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "[Content for Model Explanation]\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "[Content for Experiments]\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "[Content for Insights]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:23:48.399235: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-03 23:23:48.431690: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-03 23:23:49.023207: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "### Add the code related to the preprocessing steps in cells inside this section\n",
    "\n",
    "\"\"\"\n",
    "1.1 Import Required Libraries\n",
    "\"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning functions defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.2 Text Cleaning Functions\n",
    "\n",
    "Different models require different preprocessing strategies:\n",
    "- Twitter-RoBERTa: Specialized @user and http tokens\n",
    "- DeBERTa/XLM-RoBERTa: Basic whitespace normalization\n",
    "\"\"\"\n",
    "\n",
    "def clean_text_twitter_roberta(text):\n",
    "    \"\"\"\n",
    "    Twitter-RoBERTa specific preprocessing\n",
    "    \n",
    "    Converts:\n",
    "    - @mentions → @user token\n",
    "    - URLs → http token\n",
    "    \n",
    "    Rationale: Pre-trained model expects these specific tokens\n",
    "    \"\"\"\n",
    "    new_text = []\n",
    "    for token in text.split(\" \"):\n",
    "        # Replace @mentions\n",
    "        if token.startswith('@') and len(token) > 1:\n",
    "            token = '@user'\n",
    "        # Replace URLs\n",
    "        elif token.startswith('http'):\n",
    "            token = 'http'\n",
    "        new_text.append(token)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "\n",
    "def clean_text_basic(text):\n",
    "    \"\"\"\n",
    "    Basic preprocessing for DeBERTa and XLM-RoBERTa\n",
    "    \n",
    "    Only normalizes whitespace, preserves all content\n",
    "    Rationale: General-purpose models benefit from minimal preprocessing\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "print(\"Text cleaning functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading function defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.3 Data Loading Function\n",
    "\n",
    "Process:\n",
    "1. Load tweets from JSON\n",
    "2. Combine text with hashtags (85% contain hashtags)\n",
    "3. Apply model-specific cleaning\n",
    "4. Merge with labels and split info\n",
    "5. Separate train/test sets\n",
    "\"\"\"\n",
    "\n",
    "def load_competition_data():\n",
    "    print(\"\\n=== PREPROCESSING: Data Loading ===\")\n",
    "    \n",
    "    # Load raw tweets from JSON\n",
    "    with open('data/final_posts.json', 'r', encoding='utf-8') as f:\n",
    "        posts = json.load(f)\n",
    "    \n",
    "    # Extract and process data\n",
    "    data = []\n",
    "    for item in posts:\n",
    "        post_id = item['root']['_source']['post']['post_id']\n",
    "        text = item['root']['_source']['post']['text']\n",
    "        hashtags = item['root']['_source']['post']['hashtags']\n",
    "        \n",
    "        # Combine main text with hashtags\n",
    "        if hashtags:\n",
    "            text = text + \" \" + \" \".join([f\"#{tag}\" for tag in hashtags])\n",
    "        \n",
    "        # Apply Twitter-RoBERTa cleaning\n",
    "        # NOTE: Change to clean_text_basic() for DeBERTa/XLM\n",
    "        text_cleaned = clean_text_twitter_roberta(text)\n",
    "        \n",
    "        data.append({\n",
    "            'id': post_id,\n",
    "            'text': text_cleaned\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Load labels and split information\n",
    "    emotion_df = pd.read_csv('data/emotion.csv')\n",
    "    split_df = pd.read_csv('data/data_identification.csv')\n",
    "    \n",
    "    # Merge all information\n",
    "    df = df.merge(split_df, on='id', how='left')\n",
    "    df = df.merge(emotion_df, on='id', how='left')\n",
    "    \n",
    "    # Separate train and test sets\n",
    "    train_df = df[df['split'] == 'train'].reset_index(drop=True)\n",
    "    test_df = df[df['split'] == 'test'].reset_index(drop=True)\n",
    "    \n",
    "    # Display statistics\n",
    "    print(f\"\\nTraining samples: {len(train_df):,}\")\n",
    "    print(f\"Test samples: {len(test_df):,}\")\n",
    "    print(f\"\\nEmotion Distribution in Training Set:\")\n",
    "    print(train_df['emotion'].value_counts())\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "print(\"Data loading function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmotionDataset class defined\n"
     ]
    }
   ],
   "source": [
    "### Add the code related to the feature engineering steps in cells inside this section\n",
    "\n",
    "\"\"\"\n",
    "2.1 PyTorch Dataset Class\n",
    "\n",
    "Custom Dataset for emotion classification:\n",
    "- Handles tokenization\n",
    "- Converts text to model-ready tensors\n",
    "- Supports both training (with labels) and testing (without labels)\n",
    "\"\"\"\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts: List of text strings\n",
    "            labels: List of emotion labels (None for test set)\n",
    "            tokenizer: HuggingFace tokenizer\n",
    "            max_length: Maximum sequence length (128 covers 99% of tweets)\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        # Tokenize with padding and truncation\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,      # Add [CLS] and [SEP]\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',          # Pad to max_length\n",
    "            truncation=True,               # Truncate if longer\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Prepare return dictionary\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "        \n",
    "        # Add labels for training set\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return item\n",
    "\n",
    "\n",
    "print(\"EmotionDataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weight function defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.2 Class Weight Computation\n",
    "\n",
    "Handles class imbalance (3.7:1 ratio between most/least frequent)\n",
    "\n",
    "Strategy:\n",
    "1. Compute balanced weights inversely proportional to frequency\n",
    "2. Clip to [0.5, 3.0] to prevent over-correction\n",
    "3. Apply in loss function during training\n",
    "\"\"\"\n",
    "\n",
    "def compute_balanced_class_weights(train_df, emotions, clip_min=0.5, clip_max=3.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_df: Training DataFrame with 'emotion' column\n",
    "        emotions: List of emotion labels\n",
    "        clip_min: Minimum weight value\n",
    "        clip_max: Maximum weight value\n",
    "    \n",
    "    Returns:\n",
    "        class_weights: Tensor of weights for each class\n",
    "        label2id: Dictionary mapping emotion to index\n",
    "    \"\"\"\n",
    "    print(\"\\n=== FEATURE ENGINEERING: Class Weights ===\")\n",
    "    \n",
    "    # Create label mapping\n",
    "    label2id = {label: idx for idx, label in enumerate(emotions)}\n",
    "    train_df['label'] = train_df['emotion'].map(label2id)\n",
    "    \n",
    "    # Compute balanced weights\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.arange(len(emotions)),\n",
    "        y=train_df['label'].values\n",
    "    )\n",
    "    \n",
    "    # Clip to prevent extreme values\n",
    "    # Rationale: Extreme weights can destabilize training\n",
    "    class_weights = np.clip(class_weights, clip_min, clip_max)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "    \n",
    "    # Display computed weights\n",
    "    print(\"\\nComputed Class Weights (clipped):\")\n",
    "    for emotion, weight in zip(emotions, class_weights):\n",
    "        print(f\"  {emotion:10s}: {weight:.3f}\")\n",
    "    print()\n",
    "    \n",
    "    return class_weights, label2id\n",
    "\n",
    "\n",
    "print(\"Class weight function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration defined\n",
      "\n",
      "Key Settings:\n",
      "  Model: cardiffnlp/twitter-roberta-base-emotion\n",
      "  Epochs: 6\n",
      "  Batch Size: 8\n",
      "  Learning Rate: 2e-05\n"
     ]
    }
   ],
   "source": [
    "### Add the code related to the model implementation steps in cells inside this section\n",
    "\n",
    "\"\"\"\n",
    "3.1 Model Configuration\n",
    "\n",
    "All hyperparameters in one place for easy tuning.\n",
    "These values were selected through:\n",
    "- Literature review\n",
    "- Iterative experimentation  \n",
    "- Computational constraints\n",
    "\"\"\"\n",
    "\n",
    "class Config:\n",
    "    # Model Selection\n",
    "    MODEL_NAME = 'cardiffnlp/twitter-roberta-base-emotion'\n",
    "    # Rationale: Pre-trained on Twitter emotion data\n",
    "    \n",
    "    # Architecture\n",
    "    MAX_LENGTH = 128\n",
    "    # Rationale: Covers 99% of tweets, balances info and efficiency\n",
    "    N_FOLDS = 5\n",
    "    # Rationale: Standard for robust cross-validation\n",
    "    \n",
    "    # Training Strategy\n",
    "    EPOCHS = 6\n",
    "    # Rationale: Increased from 4 to allow better convergence\n",
    "    BATCH_SIZE = 8\n",
    "    # Rationale: GPU memory constraint (16GB)\n",
    "    LEARNING_RATE = 2e-5\n",
    "    # Rationale: Standard fine-tuning rate for transformers\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    # Rationale: L2 regularization to prevent overfitting\n",
    "    WARMUP_RATIO = 0.1\n",
    "    # Rationale: Gradual LR increase for training stability\n",
    "    EARLY_STOPPING_PATIENCE = 3\n",
    "    # Rationale: Balance between training time and convergence\n",
    "    \n",
    "    # Class Weights\n",
    "    WEIGHT_MIN = 0.5\n",
    "    WEIGHT_MAX = 3.0\n",
    "    # Rationale: Prevent extreme weight values\n",
    "    \n",
    "    # Other\n",
    "    SEED = 42\n",
    "    EMOTIONS = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
    "\n",
    "\n",
    "print(\"Configuration defined\")\n",
    "print(f\"\\nKey Settings:\")\n",
    "print(f\"  Model: {Config.MODEL_NAME}\")\n",
    "print(f\"  Epochs: {Config.EPOCHS}\")\n",
    "print(f\"  Batch Size: {Config.BATCH_SIZE}\")\n",
    "print(f\"  Learning Rate: {Config.LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WeightedTrainer class defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.2 Custom Trainer with Class Weights\n",
    "\n",
    "Extends HuggingFace Trainer to apply class weights in loss function.\n",
    "This is essential for handling the 3.7:1 class imbalance.\n",
    "\"\"\"\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Custom loss function with class weights\n",
    "        \n",
    "        Args:\n",
    "            model: The model being trained\n",
    "            inputs: Batch of training data\n",
    "            return_outputs: Whether to return model outputs\n",
    "        \n",
    "        Returns:\n",
    "            loss: Weighted cross-entropy loss\n",
    "        \"\"\"\n",
    "        # Extract labels\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Apply class weights if provided\n",
    "        if self.class_weights is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(\n",
    "                weight=self.class_weights.to(logits.device)\n",
    "            )\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "print(\"WeightedTrainer class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.3 Cross-Validation Training Function\n",
    "\n",
    "Trains 5 models using stratified K-fold cross-validation.\n",
    "\n",
    "Process:\n",
    "1. Split data into 5 stratified folds\n",
    "2. For each fold:\n",
    "   a. Train on 4 folds\n",
    "   b. Validate on 1 fold\n",
    "   c. Save best model\n",
    "3. Aggregate results for overall performance\n",
    "\n",
    "Expected runtime: 2-2.5 hours on GPU\n",
    "\"\"\"\n",
    "\n",
    "def train_with_cross_validation(train_df, config):\n",
    "    print(\"\\n=== MODEL IMPLEMENTATION: Training ===\")\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights, label2id = compute_balanced_class_weights(\n",
    "        train_df,\n",
    "        config.EMOTIONS,\n",
    "        config.WEIGHT_MIN,\n",
    "        config.WEIGHT_MAX\n",
    "    )\n",
    "    \n",
    "    id2label = {idx: label for label, idx in label2id.items()}\n",
    "    train_df['label'] = train_df['emotion'].map(label2id)\n",
    "    \n",
    "    # Initialize cross-validation\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=config.N_FOLDS,\n",
    "        shuffle=True,\n",
    "        random_state=config.SEED\n",
    "    )\n",
    "    \n",
    "    # Track predictions and scores\n",
    "    oof_preds = np.zeros(len(train_df), dtype=int)\n",
    "    cv_scores = []\n",
    "    \n",
    "    # Train each fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['label'])):\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Fold {fold + 1}/{config.N_FOLDS}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train = train_df.iloc[train_idx]['text'].values\n",
    "        y_train = train_df.iloc[train_idx]['label'].values\n",
    "        X_val = train_df.iloc[val_idx]['text'].values\n",
    "        y_val = train_df.iloc[val_idx]['label'].values\n",
    "        \n",
    "        print(f\"Train: {len(X_train):,} | Val: {len(X_val):,}\\n\")\n",
    "        \n",
    "        # Load pre-trained model and tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config.MODEL_NAME,\n",
    "            num_labels=len(config.EMOTIONS),\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            ignore_mismatched_sizes=True  # Handle label mismatch\n",
    "        )\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = EmotionDataset(X_train, y_train, tokenizer, config.MAX_LENGTH)\n",
    "        val_dataset = EmotionDataset(X_val, y_val, tokenizer, config.MAX_LENGTH)\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./results/fold_{fold}\",\n",
    "            num_train_epochs=config.EPOCHS,\n",
    "            per_device_train_batch_size=config.BATCH_SIZE,\n",
    "            per_device_eval_batch_size=config.BATCH_SIZE * 2,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            weight_decay=config.WEIGHT_DECAY,\n",
    "            warmup_ratio=config.WARMUP_RATIO,\n",
    "            logging_dir=f'./logs/fold_{fold}',\n",
    "            logging_steps=100,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1_macro\",\n",
    "            greater_is_better=True,\n",
    "            save_total_limit=1,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            dataloader_num_workers=4,\n",
    "            report_to=\"none\",\n",
    "            seed=config.SEED\n",
    "        )\n",
    "        \n",
    "        # Metrics function\n",
    "        def compute_metrics(eval_pred):\n",
    "            logits, labels = eval_pred\n",
    "            preds = np.argmax(logits, axis=-1)\n",
    "            f1_macro = f1_score(labels, preds, average='macro')\n",
    "            return {'f1_macro': f1_macro}\n",
    "        \n",
    "        # Initialize custom trainer\n",
    "        trainer = WeightedTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "            class_weights=class_weights,\n",
    "            callbacks=[\n",
    "                EarlyStoppingCallback(\n",
    "                    early_stopping_patience=config.EARLY_STOPPING_PATIENCE\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_outputs = trainer.predict(val_dataset)\n",
    "        val_preds = np.argmax(val_outputs.predictions, axis=-1)\n",
    "        oof_preds[val_idx] = val_preds\n",
    "        \n",
    "        fold_f1 = f1_score(y_val, val_preds, average='macro')\n",
    "        cv_scores.append(fold_f1)\n",
    "        \n",
    "        print(f\"\\nFold {fold + 1} F1 Score: {fold_f1:.4f}\")\n",
    "        \n",
    "        # Save model\n",
    "        os.makedirs('./best_models', exist_ok=True)\n",
    "        model.save_pretrained(f'./best_models/fold_{fold}')\n",
    "        tokenizer.save_pretrained(f'./best_models/fold_{fold}')\n",
    "        \n",
    "        # Clean up memory\n",
    "        del model, trainer, train_dataset, val_dataset\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Compute overall metrics\n",
    "    overall_f1 = f1_score(train_df['label'], oof_preds, average='macro')\n",
    "    mean_f1 = np.mean(cv_scores)\n",
    "    std_f1 = np.std(cv_scores)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Cross-Validation Results\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    for i, score in enumerate(cv_scores):\n",
    "        print(f\"Fold {i+1}: {score:.4f}\")\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"Mean F1:  {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "    print(f\"OOF F1:   {overall_f1:.4f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    print(f\"Expected Kaggle Private LB: {overall_f1:.4f} ± 0.01\")\n",
    "    \n",
    "    return overall_f1, oof_preds\n",
    "\n",
    "\n",
    "print(\"Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction function defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.4 Ensemble Prediction Function\n",
    "\n",
    "Generates predictions by ensembling all 5 fold models.\n",
    "\n",
    "Process:\n",
    "1. Load each fold's trained model\n",
    "2. Predict probabilities for test set\n",
    "3. Average probabilities across all folds\n",
    "4. Take argmax for final predictions\n",
    "\n",
    "Rationale: Averaging reduces variance and improves robustness\n",
    "\"\"\"\n",
    "\n",
    "def predict_test_ensemble(test_df, config):\n",
    "    print(\"\\n=== MODEL IMPLEMENTATION: Ensemble Prediction ===\")\n",
    "    \n",
    "    all_probs = []\n",
    "    \n",
    "    # Predict with each fold\n",
    "    for fold in range(config.N_FOLDS):\n",
    "        print(f\"\\nLoading Fold {fold + 1} model...\")\n",
    "        \n",
    "        model_path = f'./best_models/fold_{fold}'\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # Create test dataset\n",
    "        test_dataset = EmotionDataset(\n",
    "            test_df['text'].values,\n",
    "            None,  # No labels for test set\n",
    "            tokenizer,\n",
    "            config.MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=config.BATCH_SIZE * 2,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Predict probabilities\n",
    "        fold_probs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=f\"Fold {fold+1}\", leave=False):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                probs = torch.softmax(outputs.logits, dim=-1)\n",
    "                fold_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        fold_probs = np.vstack(fold_probs)\n",
    "        all_probs.append(fold_probs)\n",
    "        \n",
    "        # Clean up memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Ensemble: Average predictions from all folds\n",
    "    ensemble_probs = np.mean(all_probs, axis=0)\n",
    "    final_preds = np.argmax(ensemble_probs, axis=-1)\n",
    "    \n",
    "    # Convert numeric predictions to emotion labels\n",
    "    predictions = [config.EMOTIONS[pred] for pred in final_preds]\n",
    "    \n",
    "    print(\"\\nEnsemble prediction complete\\n\")\n",
    "    \n",
    "    return predictions, ensemble_probs\n",
    "\n",
    "\n",
    "print(\"Prediction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission function defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.5 Create Kaggle Submission File\n",
    "\n",
    "Creates properly formatted CSV file for Kaggle submission.\n",
    "\"\"\"\n",
    "\n",
    "def create_kaggle_submission(test_df, predictions, filename='submission.csv'):\n",
    "    print(\"=== Creating Submission File ===\")\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'emotion': predictions\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    submission.to_csv(filename, index=False)\n",
    "    \n",
    "    # Display statistics\n",
    "    print(f\"\\nFile saved: {filename}\")\n",
    "    print(f\"Total predictions: {len(submission):,}\\n\")\n",
    "    \n",
    "    print(\"Prediction Distribution:\")\n",
    "    dist = submission['emotion'].value_counts()\n",
    "    for emotion, count in dist.items():\n",
    "        pct = count / len(submission) * 100\n",
    "        print(f\"  {emotion:10s}: {count:5d} ({pct:5.2f}%)\")\n",
    "    \n",
    "    # Validate format\n",
    "    sample = pd.read_csv('data/samplesubmission.csv')\n",
    "    assert list(submission.columns) == list(sample.columns), \"Column mismatch\"\n",
    "    assert len(submission) == len(sample), \"Row count mismatch\"\n",
    "    print(\"\\n✓ Format validation passed\\n\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "\n",
    "print(\"Submission function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DM2025 Lab 2 - Phase 3: Complete Pipeline\n",
      "======================================================================\n",
      "\n",
      "Improvements Applied:\n",
      "  1. Model: cardiffnlp/twitter-roberta-base-emotion\n",
      "  2. Epochs: 6 (increased from 4)\n",
      "  3. Batch Size: 8\n",
      "  4. Learning Rate: 2e-05\n",
      "  5. Weight Clipping: [0.5, 3.0]\n",
      "  6. Text Cleaning: @user and http token preservation\n",
      "  7. Early Stopping Patience: 3\n",
      "\n",
      "=== PREPROCESSING: Data Loading ===\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/final_posts.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  7. Early Stopping Patience: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mConfig.EARLY_STOPPING_PATIENCE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Step 1: Preprocessing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m train_df, test_df = \u001b[43mload_competition_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Step 2: Model Training\u001b[39;00m\n\u001b[32m     30\u001b[39m oof_f1, oof_preds = train_with_cross_validation(train_df, Config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mload_competition_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== PREPROCESSING: Data Loading ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Load raw tweets from JSON\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/final_posts.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     17\u001b[39m     posts = json.load(f)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Extract and process data\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/11401dmlab2/lib/python3.11/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/final_posts.json'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main Execution\n",
    "\n",
    "This will:\n",
    "1. Load and preprocess data\n",
    "2. Train 5 models with cross-validation (2-2.5 hours on GPU)\n",
    "3. Generate ensemble predictions\n",
    "4. Create submission.csv\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DM2025 Lab 2 - Phase 3: Complete Pipeline\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nImprovements Applied:\")\n",
    "print(f\"  1. Model: {Config.MODEL_NAME}\")\n",
    "print(f\"  2. Epochs: {Config.EPOCHS} (increased from 4)\")\n",
    "print(f\"  3. Batch Size: {Config.BATCH_SIZE}\")\n",
    "print(f\"  4. Learning Rate: {Config.LEARNING_RATE}\")\n",
    "print(f\"  5. Weight Clipping: [{Config.WEIGHT_MIN}, {Config.WEIGHT_MAX}]\")\n",
    "print(f\"  6. Text Cleaning: @user and http token preservation\")\n",
    "print(f\"  7. Early Stopping Patience: {Config.EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "# Step 1: Preprocessing\n",
    "train_df, test_df = load_competition_data()\n",
    "\n",
    "# Step 2: Model Training\n",
    "oof_f1, oof_preds = train_with_cross_validation(train_df, Config)\n",
    "\n",
    "# Step 3: Ensemble Prediction\n",
    "predictions, probs = predict_test_ensemble(test_df, Config)\n",
    "\n",
    "# Step 4: Create Submission\n",
    "submission = create_kaggle_submission(test_df, predictions, 'submission.csv')\n",
    "\n",
    "# Final Summary\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Training Complete\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Local OOF F1: {oof_f1:.4f}\")\n",
    "print(f\"  Expected Kaggle Private LB: {oof_f1:.4f} ± 0.01\")\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  1. submission.csv\")\n",
    "print(f\"  2. ./best_models/fold_0-4/\")\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"  1. Submit submission.csv to Kaggle\")\n",
    "print(f\"  2. Compare with baseline (0.6844)\")\n",
    "print(f\"  3. Max 5 submissions per day\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dm2025lab02)",
   "language": "python",
   "name": "dm2025lab02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
